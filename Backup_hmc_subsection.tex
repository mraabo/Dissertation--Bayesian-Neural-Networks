\section{Hamiltonian Monte Carlo}
As we mention in section \ref{sec:Metropolis_Hastings}, a significant inefficiency in the Metropolis algorithm, is that it often behaves like a random walk and thus the simulations take a longer road while moving through the target distribution, which gives rise to slow simulation. This is especially a problem when concerning high-dimensional problems, such as Bayesian Neural Networks (\cite{neal2012bayesian}).
Another way to generate proposals with a higher efficiency, is by updating the parameters dynamically by simulating Hamiltonian dynamics and then use the Metropolis algorithm to accept or reject these proposals. Such an algorithm suppress the local random walk behavior and thus allowing it to move much faster and more rapidly through the distribution. This method, which we will present in this chapter, is called Hamilton Monte Carlo (HMC), and is commonly used in computational physics and statistics. The algorithm was originally proposed by \cite{Duane1987216} for calculations used in lattice quantum chromodynamics, but was later introduced to the field of computation statistics when it was used for Bayesian Neural Networks n \cite{neal2012bayesian}\\
\\
The HMC algorithm reduces correlation between successive sampled states, compared to the Metropolis Hastings algorithm in section \ref{sein tc:Metropolis_Hastings}, by proposing moves to distant states which maintain a high probability of acceptance due to the approximate energy conserving properties of the simulated Hamiltonian dynamic when using a symplectic integrator\footnote{In mathematics, a symplectic integrator is a numerical integration scheme for Hamiltonian systems.}. The reduced correlation means fewer Markov chain samples are needed to approximate integrals with respect to the target probability distribution for a given Monte Carlo error. For the HMC algorithm we need to construct a differential equation system, that is known to keep the target distribution invariant. If we can achieve this, we can simulate transitions following the solution to this differential equation system and these transitions can be shown to leave the target distribution invariant. Such a differential equations system is often the Hamiltonian dynamics named after William Rowan Hamilton and will be introduced in the next section. 

\subsection{Hamiltonian dynamics}
Before we move to the actual HMC algorithm, we need to become familiar with the concept of Hamiltonian dynamics. Hamiltonian dynamics are used to describe how an object move around in a system. The Hamiltonian dynamics are defined by the objects position $\boldsymbol{\theta}\in \mathbb{R}^d$ and it's momentum $\boldsymbol{\rho}\in \mathbb{R}^d$, which in the physics are equivalent to an object's mass times it's velocity at some point in time. The object's position is associated with a potential energy $U(\boldsymbol{\theta})$ and likewise the momentum is associated with a kinetic energy $K(\boldsymbol{\rho})$. The sum of the potential and kinetic energy is regarded as the total energy of the system and often called the Hamiltonian,
\begin{equation*}
H(\boldsymbol{\theta},\boldsymbol{\rho})=U(\boldsymbol{\theta})+K(\boldsymbol{\rho})    
\end{equation*}       
one important feature of the Hamiltonian is that it is constant over time. Taking the partial derivative with respect to time of the position and momentum, shows us how they evolve over time,
\begin{equation}\label{eq:hamilton_equations}
\begin{split}
\frac{d \theta_{i}}{d t}=\frac{\partial H}{\partial \rho_{i}}=\frac{\partial K(,\boldsymbol{\rho})}{\partial \rho_{i}} \\
\frac{d \rho_{i}}{d t}=-\frac{\partial H}{\partial \theta_{i}}=-\frac{\partial U(,\boldsymbol{\theta})}{\partial \theta_{i}}
\end{split}
\end{equation}
where $i=1,2, \ldots,d$. These are named Hamiltonian equations and represents differential equations system. The Hamiltonian equations are very useful, since if we are able to evaluate the partial derivatives from equation \ref{eq:hamilton_equations}, we are able to predict the position and momentum variable of the object at any point in the future $t^\prime>t$.
\subsection{Properties of the Hamiltonian Dynamics}
The Hamiltonian dynamics have several properties, which are very crucial when used in relation to MCMC simulations.\\
\\
\textit{Property of time reversibility:}\\
This property is very important for showing that the HMC transitions, generated by the dynamics, leave the target distribution invariant, since the reversibility of the chain transitions requires reversibility of the dynamics. \\
\\
\textit{Property of conservation:}\\
Secondly we have the property of conservation, that is the dynamics keeps the Hamiltionian invariant. This can easily be verified from equation \ref{eq:hamilton_equations}
\begin{equation}\label{eq:Hamilton_conservation}
    \frac{d \H}{d t}=\sum_{i=1}^{d}\left[\frac{d \theta_{i}}{d t} \frac{\partial \H}{\partial \theta_{i}}+\frac{d \rho_{i}}{d t} \frac{\partial \H}{\partial \rho_{i}}\right]=\sum_{i=1}^{d}\left[\frac{\partial \H}{\partial \rho_{i}} \frac{\partial \H}{\partial \theta_{i}}-\frac{\partial \H}{\partial \theta_{i}} \frac{\partial \H}{\partial \rho_{i}}\right]=0
\end{equation}
In the HMC method, we will use Metropolis updates, with proposals found by the Hamiltonian dynamic. We have that the acceptance probability is one if $\H$ is kept invariant (see e.g \cite{neal2012mcmc}). This is however not possible in practise, and we are only able to keep $\H$ approximately invariant, but we will come back to this issue later. \\
\\
\textit{Property of volume preservation:}
The last crucial property of the Hamiltonian dynamics is the volume preservation property, which gives us that the dynamics preserves volume in the $(q,p)$ space.

\begin{equation*}
    \sum_{i=1}^{d}\left[\frac{\partial}{\partial \theta_{i}} \frac{d \theta_{i}}{d t}+\frac{\partial}{\partial \rho_{i}} \frac{d \rho_{i}}{d t}\right]=\sum_{i=1}^{d}\left[\frac{\partial}{\partial \theta_{i}} \frac{\partial \H}{\partial \rho_{i}}-\frac{\partial}{\partial \rho_{i}} \frac{\partial \H}{\partial \theta_{i}}\right]=\sum_{i=1}^{d}\left[\frac{\partial^{2} \H}{\partial \theta_{i} \partial \rho_{i}}-\frac{\partial^{2} \H}{\partial \rho_{i} \partial \theta_{i}}\right]=0
\end{equation*}

\subsection{Discretizing Hamiltonian Equations}
The Hamiltonian equations describe how an objective evolve in continuous time, but for simulating Hamiltonian dynamics on a computer system, we have to approximate the differential equations numerically in some way, this is done by discretizing time. We do this by splitting the time interval $dt$ into small intervals $\varepsilon$. We will now present the two most common ways of handling this. The first is the Euler's Method and lastly the Leapfrog Method.
\subsubsection*{The Euler’s Method}
This is one of the most common methods in computational science for solving a differential equation system. For Hamiltonian equations the Euler methods update the position and momentum variable as follows,

\begin{equation*}
\begin{split}
\rho_{i}(t+\varepsilon)=\rho_{i}(t)+\varepsilon \frac{d \rho_{i}}{d t}(t)=\rho_{i}(t)-\varepsilon \frac{\partial U}{\partial \theta_{i}(t)} \\
\theta_{i}(t+\varepsilon)=\theta_{i}(t)+\varepsilon \frac{d \theta_{i}}{d t}=\theta_{i}(t)+\varepsilon \frac{\partial K}{\partial \rho_{i}(t)}
\end{split}
\end{equation*}
According to \cite{neal2012mcmc} a slightly better result can be obtained by,
\begin{equation*}
\begin{split}
\rho_{i}(t+\varepsilon) &=\rho_{i}(t)-\varepsilon \frac{\partial U}{\partial \theta_{i}}(\theta(t)) \\
\theta_{i}(t+\varepsilon) &=\theta_{i}(t)+\varepsilon \frac{\rho_{i}(t+\varepsilon)}{m_{i}}
\end{split}
\end{equation*}
\subsubsection*{The Leapfrog method}
Another discretizing scheme often used for simulation Hamiltonian equations, is the Leapfrog method. Whereas the Euler’s method takes full steps for updating position and momentum, the leapfrog method takes a half steps to update momentum value,
\begin{equation*}
\begin{split}
\rho_{i}(t+\varepsilon / 2)=\rho_{i}(t)-(\varepsilon / 2) \frac{\partial U}{\partial \theta_{i}(t)} \\
\theta_{i}(t+\varepsilon)=\theta_{i}(t)+\varepsilon \frac{\partial K}{\partial \rho_{i}(t+\varepsilon / 2)} \\
\rho_{i}(t+\varepsilon)=\rho_{i}(t)-(\varepsilon / 2) \frac{\partial U}{\partial \theta_{i}(t+\varepsilon)}
\end{split}
\end{equation*}
According to \cite{neal2012mcmc} the Leapfrog method, preserves volume exactly, \textcolor{red}{which is good??} and it is also reversible. 

\subsection{Hamiltonian and Probability: Canonical Distributions}
We now have a slightly better understanding understanding of what is a Hamiltonian and how we can simulate it's dynamics by either the Euler method or the Leapfrog method. We now need to connect this with the MCMC theory from the previous sections. In order to perform this connection, we need to relate the target distribution $\hat{p}(\boldsymbol{\theta})$ and the Hamiltonian, such that we can use the Hamiltonian equations to the target distribution. A way of doing this, proposed by \cite{neal2012bayesian}, is use a concept from statistical mechanics known as the canonical (Boltzman) distribution. The probability distribution of $\boldsymbol{\theta}$ under the canonical distribution can be written as
\begin{equation*}
    \hat{p}(\boldsymbol{\theta})\propto \exp(\frac{-U(\boldsymbol{\theta})}{T})
\end{equation*}
where $U(\boldsymbol{\theta})$ again is the potential energy. $T$ is often called the temperature of the system and usually chosen to be equal to one (\cite{neal2012bayesian}).
One should note that any probability distribution that is nowhere zero can be put into this form by letting $U(\boldsymbol{\theta})=-\log \hat{p}(\boldsymbol{\theta})-\log Z$, for any convenient choice of $Z$.  Since the Hamiltonian is an energy function for the joint state of both position and momentum, we need a joint distribution, which is given by
\begin{equation*}
p(\boldsymbol{\theta},\boldsymbol{\rho})\propto \exp(-\H(\boldsymbol{\theta},\boldsymbol{\rho}))   = \exp(-U(\boldsymbol{\theta}))\exp(-K(\boldsymbol{\rho}))=\hat{p}(\boldsymbol{\theta})p(\boldsymbol{\rho})
\end{equation*}
where the last equality hold since we assume independence between $\boldsymbol{\theta}$ and $\boldsymbol{\rho}$. We now have a joint distribution, in terms of the Hamiltonian function, which we know how to simulate. But we are in fact only interested in the position variable $\boldsymbol{\theta}$ and not the momentum variable $\boldsymbol{\rho}$, which we in some way can interpret as a "helper" variable that enable us to simulate the joint distribution. In order to obtain marginal samples from the target distribution only, on can simply throw away the samples the momentum distribution, because they are independent anyway. Since we can interpret the momentum variable as a  helper variable, we are allowed freely to decide the marginal distribution $p(\boldsymbol{\rho})$. The literature often choose it to be Gaussian, $\boldsymbol{\rho}\sim \mathcal{N}\left(0, \Sigma \right)$, where $\Sigma$ is some symmetric, positive-definite mass matrix and often chosen to be diagonal, such that $\boldsymbol{\rho}$ is d-dimensional multivariate normal where the variables are independent. 
The Hamiltonian equations from equation \ref{eq:hamilton_equations} can now be written as,
\begin{equation*}
\begin{split}
\frac{d \theta_{i}}{d t}&=\left[\Sigma^{-1}\rho\right]_i \\
\frac{d \rho_{i}}{d t}&=-\frac{\partial U}{\partial \theta_i}
\end{split}
\end{equation*}





\subsection{Hamiltonian Monte Carlo}
We start the HMC algorithm from an initial state $\boldsymbol{\theta}_0$ $\boldsymbol{\rho}_0$, and then we simulate the Hamiltonian dynamics for $t+\varepsilon$ using the Leapfrog method. We choose the Leapfrog method since \cite{betancourt2017conceptual} shows it to be more effective. The states generated for the position and momentum variable at the end of the Leapfrog simulation is used as proposals for a new state $(\boldsymbol{\theta}^\prime,\boldsymbol{\rho}^\prime)$. The proposed stats are accepted according to the Metropolis acceptance criteria,
\begin{equation*}
\begin{split}
    \alpha\left((\boldsymbol{\theta},\boldsymbol{\rho}) \mapsto (\boldsymbol{\theta}^\prime , \boldsymbol{\rho}^\prime )\right) &= \min\left\{1, \frac{p(\boldsymbol{\theta}^\prime,\boldsymbol{\rho}^\prime)}{p(\boldsymbol{\theta},\boldsymbol{\rho})} \right\}\\
    &= \min\{1,\exp\left(\log p(\boldsymbol{\theta}^\prime,\boldsymbol{\rho}^\prime)- \log p(\boldsymbol{\theta}, \boldsymbol{\rho})  \right)\\
    &= \min \left\{1,\exp\left(-\H(\boldsymbol{\theta}^\prime,\boldsymbol{\rho}^\prime) +\H(\boldsymbol{\theta},\boldsymbol{\rho})\right) \right\}
\end{split}
\end{equation*}
If we could simulate the Hamiltonian dynamics exactly, the Metropolis acceptance criteria would always be equal to one, due to the Hamiltonian conservation criteria in equation \ref{eq:Hamilton_conservation} which would give $\min \{1, \exp (0)\}=1$. But since we cannot simulate the Hamilton dynamics exactly and we need to approximate them with the Leapfrog scheme introduced earlier. We can however see that if we choose a proper way of discretize the dynamics, the term  $\H(\boldsymbol{\theta},\boldsymbol{\rho})-\H(\boldsymbol{\theta}^\prime,\boldsymbol{\rho}^\prime)$ in the exponent should be small, thus a high acceptance rate. This is a very clever way of making proposals, since we can make very large and uncorrelated moves in the state space, while keeping a high acceptance of probability. The algorithm is written in pseudo code in algorithm \ref{alg:HMC}. \\
\\
The Algorithm relies heavily on choosing values for $\varepsilon$ and $L$. If we pick a too large value for $\varepsilon$, then the simulations will be inaccurate and give us a low acceptance rate. If we on the other hand choose $\varepsilon$ which is too small, we will waste computation by taking small steps. A too small $L$ will give us successive samples that lies close to each other, which gives us the same undesirable random walk behavior as with the Metropolis algorithm from section \ref{sec:Metropolis_Hastings}. A too large choice for $L$, will produce trajectories that loops back and retrace their steps again and even worse if we choose $L$ such that the samples jumps from one side of the space to the other in each iteration, then accoding to.


\begin{algorithm}[h!]\label{alg:HMC}

\SetAlgoLined
$\text{Given } \boldsymbol{\theta}^{0}, \varepsilon, L, \mathcal{L}, M$:\\
\For{m=1 to M}{
Sample $\boldsymbol{\rho}^{0} \sim \mathcal{N}(0, I)$ \\
Set $\boldsymbol{\theta}^{m} \leftarrow \boldsymbol{\theta}^{m-1}, \boldsymbol{\theta}^\prime \leftarrow \boldsymbol{\theta}^{m-1}, \boldsymbol{\rho}^\prime \leftarrow \boldsymbol{\rho}^{0}$\\
\For{i = 1 to L}{
Set $\boldsymbol{\theta}^\prime, \boldsymbol{\rho}^\prime \leftarrow \operatorname{Leapfrog}(\boldsymbol{\theta}^\prime, \boldsymbol{\rho}^\prime, \varepsilon)$\\
With probability $\alpha=\min \left\{1, \frac{\exp \left\{\mathcal{L}(\boldsymbol{\theta}^\prime)-\frac{1}{2} \boldsymbol{\rho}^\prime \cdot \boldsymbol{\rho}^\prime\right\}}{\exp \left\{\mathcal{L}\left(\boldsymbol{\theta}^{m-1}\right)-\frac{1}{2} \boldsymbol{\rho}^{0} \cdot \boldsymbol{\rho}^{0}\right\}}\right\}, \text { set } \boldsymbol{\theta}^{m} \leftarrow \boldsymbol{\theta}^\prime, \boldsymbol{\rho}^{m} \leftarrow-\boldsymbol{\rho}^\prime$

}
}

\SetKwFunction{Lepfrog}{Lepfrog}
\SetKwProg{Fn}{Function}{:}{\KwRet{$\boldsymbol{\theta}^\prime,\boldsymbol{\rho}^\prime$}}
\Fn{\Lepfrog{$\boldsymbol{\theta}$, $\boldsymbol{\rho}$, $\varepsilon$}}{
 Set $\boldsymbol{\rho}^\prime \leftarrow r+(\varepsilon / 2) \nabla_{\theta} \mathcal{L}(\theta)$\\
Set $\boldsymbol{\theta}^\prime \leftarrow \theta+\varepsilon \boldsymbol{\rho}^\prime$\\
Set  $\boldsymbol{\rho}^\prime \leftarrow \boldsymbol{\rho}^\prime+(\varepsilon / 2) \nabla_{\theta} \mathcal{L}(\boldsymbol{\theta}^\prime)$
  }
\caption{Hamiltonian Monte Carlo}
\end{algorithm}







\subsection{No-U-Turn Hamiltonian Monte Carlo}
No-U-Turn Sampler (NUTS) introduced by \cite{hoffman2011nouturn}
extends the HMC by eliminating the need to specify the trajectory length $L$.

