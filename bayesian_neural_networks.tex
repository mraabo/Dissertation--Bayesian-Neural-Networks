\chapter{Bayesian Neural Networks}
\clearpage
\section{Priors}
\section{Markov Chain Monte Carlo}\label{sec:MCMC}
How to sample from an intractable posterior distribution has been examined deeply in the litterateur through out the years and the most general methods is the Markov Chain Monte Carlo (MCMC) method, which is based on drawing values of $\boldsymbol{\theta}$ from some proposed distribution $q(\boldsymbol{\theta})$ and then correcting the draws in such a way that the approximate the target distribution $p(\boldsymbol{\theta}|S)$ well. Before we go any further into the different MCMC methods, we need some basic knowledge of Markov Chains. A Markov Chain is per definition a sequence of random variables (a stochastic process) $\boldsymbol{\theta}^{(1)},\boldsymbol{\theta}^{(2)}, \ldots$ for which the conditional probability of being at a particular state at $\boldsymbol{\theta}^{(t)}$, depends only on the previous state in the sequence $\boldsymbol{\theta}^{(t-1)}$. This property is also known as the Markov Property. A Markov Chain is uniquely defined by it's transition probabilities $T_t(\boldsymbol{\theta}^{(t)}|\boldsymbol{\theta}^{(t-1)}) $, which is the probability of transitioning from one state in the chain to another. In this context we are in particular interested in what happens with the chain as the number of steps goes infinity. In the connection with MCMC methods, we are especially interested in chains that converges to specific distribution. On the other hand chains that diverge, are not usually not appropriate to use in the context of MCMC methods, as these do not follow as systematic dynamic and will typically explode. We say that a Markov Chain is stationary if
\begin{equation*}
    \left(\boldsymbol{\theta}^{(0)},\boldsymbol{\theta}^{(1)}, \cdots, \boldsymbol{\theta}^{(n)}\right) \sim\left(\boldsymbol{\theta}^{(k)}, \boldsymbol{\theta}^{(k+1)}, \cdots, \boldsymbol{\theta}^{(k+n)}\right) \forall \text{ } n \text{ and } k \in \mathbb{N}
\end{equation*}
The fact that the Markov Chain is stationary means that all the elements in the chain follows the same distribution regardless of what state the chain is in. In particular for a stationary distribution is that the distribution of $\boldsymbol{\theta}^t$ is the same for all $t$. \\
\\
Another type of Markov Chains that we are interested in, is one that complies with the follow reversibility constraint
\begin{equation*}
    p(\boldsymbol{\theta}^{(t)})T\left(\boldsymbol{\theta}^{(t)},\boldsymbol{\theta}^{(t-1)}\right)=p(\boldsymbol{\theta}^{(t-1)})T\left(\boldsymbol{\theta}^{(t-1)},\boldsymbol{\theta}^{(t)}\right)
\end{equation*}
the reversibility constraint is also called the detailed balance condition, and states that if the chain is reversed it is the same process as the forward process. In the context of MCMC, it turns out that all constructed reversible Markov chains will converge towards the target distribution. In other words, this means that all reversible Markov chains will have a stationary distribution. Reversible Markov chains are particularly useful in relation to the MCMC methods, because the detailed balanced condition for a desired target distribution $p(\cdot)$ implies that the Markov chain is constructed in such a way that $p(\cdot)$ turns out to be the stationary distribution. This follows directly from theorem \ref{theo:detailed_balance} below.
\begin{mytheorem}\label{theo:detailed_balance}
Let $T$ be a transition probability which satisfies the detailed balance condition $p(x)T(x, y) = p(y)T(y, x)$ with the function $p$ defined as the probability distribution, then the following holds:
\begin{enumerate}
    \item $p(\cdot)$ is a stationary distribution of the Markov chain associated with the trainsition kernel $T(\cdot)$
    \item The Markov chain is reversible
\end{enumerate}
\end{mytheorem}
If we can construct a transition probability $T$ that satisfies the detailed balance condition and run the chain for a long period, samples generated from this distribution will eventually end up being drawn from the target distribution. 
We will not go much deeper into litterateur of Markov Chains and stochastic processes in this dissertation and the curious reader is therefor encourage to see e.g \cite{lawler2006introduction} for further information on this topic. 
 
\section{Bayesian Neural Network with rejection sampling}
\textcolor{red}{Tænker at vi skal have dit eksempel med, som du konstruerede ud fra NEAL - men ved ikke lige hvor det passer ind henne? Skal vi have en section til det eller skal vi forsøge at presse det ind et sted? Vil du kigge på det ved lejlighed?}
\section{Metropolis Hastings algorithm} \label{sec:Metropolis_Hastings}

The Metropolis-Hastings algorithm, originally Metropolis, is a Markov chain Monte Carlo (MCMC) method, which is used to to sample from a Bayesian posterior. \\
The Metropolis-Hastings (MH) algorithm was originally introduced by \cite{Metropolis1953} and was developed to simulate the states for a system of idealized molecules (\cite{neal2012mcmc}). This was later further developed by \cite{hastings70}, so that the algorithm could now simulate from a general distribution and not just a symmetric one, as it was previously based on. Due to its versatility and simplicity, the Metropolis-Hasting algorithm is one of the most widely used in MCMC methods.
\\
The Metropolis-Hastings algorithm generates a sequence of stochastic variables by sampling from a probability distribution. Samples are made from an proposal distribution, as the target distribution, normally the posterior, often is intractable. Central to the algorithm is that it is based on the basic Markov chain theory defined in section \ref{sec:MCMC}.\\
\\
The main idea is to create a Markov chain that will converge towards a given distribution, so that the correct distribution becomes the Markov chain's stationary distribution. This is done by the extraction of samples from the Markov chain's stationary distribution can be approximated by simulating the Markov chain. The simple Metropolis algorithm introduced in the original paper by \cite{Metropolis1953} consider a target distribution $p(\boldsymbol{\theta})$ and a proposal distribution $q(\boldsymbol{\theta})$. The algorithm generates a Markov chain, where a new state $\boldsymbol{\theta}^{(t+1)}$ is produced by the the previous state $\boldsymbol{\theta}^t$, where we first generate a candidate state $\boldsymbol{\theta}^{cand}$ and decide whether or not to reject this candidate state based on the relative probability density to the old state. If the relative density is larger than one, we accept the new state, if the relative density is less than one, we accept the new state with probability $\frac{p(\boldsymbol{\theta}^{cand})}{p(\boldsymbol{\theta}^{t})}$. The algorithm is written i pseudo code in algorithm \ref{algo_2}.

% Metropolis Algorithm
\begin{algorithm}[H]\label{algo_2}
\SetAlgoLined
initialize $\boldsymbol{\theta}^0\sim q(\boldsymbol{\theta})$;

\For{$i=1,2,\ldots$}{
Propose: $\boldsymbol{\theta}^{c a n d} \sim q\left(\boldsymbol{\theta}^{(i)} \mid \boldsymbol{\theta}^{(i-1)}\right)$

Acceptance Probability:

$ \alpha\left(\boldsymbol{\theta}^{c a n d} \mid \boldsymbol{\theta}^{(i-1)}\right)=\min \left\{1, \frac{p\left(\boldsymbol{\theta}^{c a n d}\right)}{ p\left(\boldsymbol{\theta}^{(i-1)}\right)}\right\} $

$u \sim  \text { Uniform }(0,1)
$

  \uIf{$u<\alpha$}{
    Accept the proposal: $\boldsymbol{\theta}^{(i)} \leftarrow \boldsymbol{\theta}^{c a n d}$\;
  }
  \Else{
    Reject the proposal: $\boldsymbol{\theta}^{(i)} \leftarrow \boldsymbol{\theta}^{(i-1)}$ \;
  }
    }
\caption{Metropolis algorithm}
\end{algorithm}
In the original algorithm the proposal distribution must satisfy the symmetry condition,
\begin{equation*}
    q(\boldsymbol{\theta'}\mid \boldsymbol{\theta})=q(\boldsymbol{\theta}\mid \boldsymbol{\theta'})
\end{equation*}
To show that this works out, and we are in fact sampling from the target distribution, we need to show that these transitions from one state to another in the Markov chain leaves the target distribution invariant. For $\boldsymbol{\theta'}\neq \boldsymbol{\theta}$ the algorithm leads to the following transitions densities,
\begin{equation*}
    T\left(\boldsymbol{\theta}' \mid \boldsymbol{\theta}\right)=q\left(\boldsymbol{\theta}' \mid \boldsymbol{\theta}\right) \min \left(1, p\left(\boldsymbol{\theta}'\right) / p(\boldsymbol{\theta})\right)
\end{equation*}
The detailed balance condition from theorem \ref{theo:detailed_balance} is verified by,
\begin{equation*}
\begin{aligned}
T\left(\boldsymbol{\theta}' \mid \boldsymbol{\theta}\right) p(\boldsymbol{\theta}) &=q\left(\boldsymbol{\theta}' \mid \boldsymbol{\theta}\right) \min \left(1, p\left(\boldsymbol{\theta}'\right) / p(\boldsymbol{\theta})\right) p(\boldsymbol{\theta}) \\
&=q\left(\boldsymbol{\theta}' \mid \boldsymbol{\theta}\right) \min \left(p(\boldsymbol{\theta}), p\left(\boldsymbol{\theta}'\right)\right) \\
&=q\left(\boldsymbol{\theta} \mid \boldsymbol{\theta}'\right) \min \left(p\left(\boldsymbol{\theta}'\right), p(\boldsymbol{\theta})\right) \\
&=q\left(\boldsymbol{\theta} \mid \boldsymbol{\theta}'\right) \min \left(1, p(\boldsymbol{\theta}) / p\left(\boldsymbol{\theta}'\right)\right) p\left(\boldsymbol{\theta}'\right) \\
&=T\left(\boldsymbol{\theta} \mid \boldsymbol{\theta}'\right) p\left(\boldsymbol{\theta}'\right)
\end{aligned}
\end{equation*}
the transitions proposed by the algorithm thus leaves the target distribution $p(\boldsymbol{\theta})$ invariant and thus samples produced by this Markov chain all has the same stationary distribution $p(\boldsymbol{\theta})$. One problem arises since the algorithm needs to calculate the relative density of $\frac{p(\boldsymbol{\theta'})}{p\left(\boldsymbol{\theta}\right)}$, but our target distribution is often intractable and thus a analytical solution is not possible. But since the algorithm only need the target distribution for calculating the relative density, we can use that we know that the posterior is equal to the likelihood time the prior up to some constant of normalization and thus we can use this instead of the target distribution directly.\\
\\
A more generalized version of the algorithm is the one introduced by \cite{hastings70}, which allows for non-symmetric proposal distributions. The algorithm is written in pseudo code in algorithm \ref{algo_3}.

% Metropolis Hasting Algorithm
\begin{algorithm}[H]\label{algo_3}
\SetAlgoLined
initialize $\boldsymbol{\theta}^0\sim q(\boldsymbol{\theta})$;

\For{$i=1,2,\ldots$}{
Propose: $\boldsymbol{\theta}^{c a n d} \sim q\left(\boldsymbol{\theta}^{(i)} \mid \boldsymbol{\theta}^{(i-1)}\right)$

Acceptance Probability:

$ \alpha\left(\boldsymbol{\theta}^{c a n d} \mid \boldsymbol{\theta}^{(i-1)}\right)=\min \left\{1, \frac{q\left(\boldsymbol{\theta}^{(i-1)} \mid \boldsymbol{\theta}^{c a n d}\right) p\left(\boldsymbol{\theta}^{c a n d}\right)}{q\left(\boldsymbol{\theta}^{c a n d} \mid \boldsymbol{\theta}^{(i-1)}\right) p\left(\boldsymbol{\theta}^{(i-1)}\right)}\right\} $

$u \sim  \text { Uniform }(0,1)
$

  \uIf{$u<\alpha$}{
    Accept the proposal: $\boldsymbol{\theta}^{(i)} \leftarrow \boldsymbol{\theta}^{c a n d}$\;
  }
  \Else{
    Reject the proposal: $\boldsymbol{\theta}^{(i)} \leftarrow \boldsymbol{\theta}^{(i-1)}$ \;
  }
    }
    
 
\caption{Metropolis-Hastings algorithm}
\end{algorithm}









\section{Hamiltonian Monte Carlo}
A significant inefficiency in the Metropolis-Hastings algorithm, as stated in section \ref{sec:Metropolis_Hastings}, is that it often behaves like a random walk and thus the simulations take a longer road while moving through the target distribution, which gives rise to slow simulation. Reparameterization and efficient jumping rules can improve this, as proposed in \cite{gelmanbda04}, but is often not sufficient for complicated model with high-dimensional target distributions as we face with Bayesian Neural Networks. We will in this present a Markov chain Monte Carlo implementation for Bayesian Neural networks, in which the network weights are updated dynamically by simulating Hamiltonian dynamics and rejecting simulated proposals, by some form of the Metropolis-Hastings algorithm. This algorithm suppress the local random walk behavior and thus allowing it to move much faster and rapidly through the distribution. The method presented is called Hamilton Monte Carlo (also known as hybrid Monte Carlo), and is commonly used in computational physics and statistics. The algorithm was originally proposed by \cite{Duane1987216} and used in the field of quantum chromodynamics, but was later introduced to the world of statistics by \cite{neal2012bayesian}. The HMC algorithm reduces correlation between successive sampled states, compared to the Metropolis Hastings algorithm in section \ref{sec:Metropolis_Hastings}, by proposing moves to distant states which maintain a high probability of acceptance due to the approximate energy conserving properties of the simulated Hamiltonian dynamic when using a symplectic integrator\footnote{In mathematics, a symplectic integrator is a numerical integration scheme for Hamiltonian systems.}. The reduced correlation means fewer Markov chain samples are needed to approximate integrals with respect to the target probability distribution for a given Monte Carlo error.

\subsection{Hamiltonian Dynamics}
Hamiltonian dynamics is a classical physical system described by a set of canonical coordinates $r = (q, p)$, where each component of the coordinate $q_i$, $p_i$ is indexed to the frame of reference of the system. The $q_i$ are called generalized coordinates, and are chosen so as to eliminate the constraints or to take advantage of the symmetries of the problem, and $p_i$ are their conjugate momenta. The system is described by a function $\mathcal{H}(q,p,t)$ known as the Hamiltonian, which corresponds to the total energy of the system. The partial derivatives of the Hamiltonian with respect to time, t, denotes how $q$ and $p$ changes over time
\begin{equation}\label{eq:hamilton_equations}
\begin{split}
\frac{d q_{i}}{d t}&=\frac{\partial \mathcal{H}}{\partial p_{i}} \\
\frac{d p_{i}}{d t}&=-\frac{\partial \mathcal{H}}{\partial q_{i}}
\end{split}
\end{equation}
for $i=1,2,d$. For any time interval of duration $s$, these equations define a mapping from the state at any time $t$ to the state at time $t + s$.


\subsection{Potential and Kinetic Energy}
We wish to sample from the distribution $p(\boldsymbol{q})$, which has $d$ real valued components $q_1,q_2,\ldots,q_d$. In our case $\boldsymbol{q}$ corresponds to the set of network parameters $\boldsymbol{\theta}$, but since the litterateur is often referring to $q$ we will keep this notation. The probability for this variable under the canonical distribution is
\begin{equation*}
    p(q)\propto \exp(-E(q))
\end{equation*}
where $E(q)$ is the potential energy function. We have followed the procedure as in \cite{neal2012bayesian} and set the temperature parameter of the canonical distribution to one. One should note that any probability distribution that is nowhere zero can be but in to this form by lettng $E(q)=-\log p(q)-\log Z$, for any convenient choice of $Z$. The canonical distribution of $q$ and $p$ together is

\begin{equation*}
p(q,p)\propto \exp(-\mathcal{H}(q,p))    
\end{equation*}
whereas for a closed system the total energy $\mathcal{H}$ can be written as 
\begin{equation*}
    \mathcal{H}(q,p)=U(q)+K(p)
\end{equation*}
where U(q) is the potential energy and K(p) is the kinetic energy. Often the kinetic energy can be written as
\begin{equation*}
    K(p)=p^\top M^{^-1}p/2
\end{equation*}
where $M$ is some symmetric, positive-definite mass matrix and often chosen to be diagonal. This correspondents to choosing the kinetic energy to be equal the log probability of a zero-mean Gaussian distribution, ignoring the constant of normalization, with covariance matrix $M$. The Hamiltonian equations from equation \ref{eq:hamilton_equations} can now be written as,
\begin{equation*}
\begin{split}
\frac{d q_{i}}{d t}&=\left[M^{-1}p\right]_i \\
\frac{d p_{i}}{d t}&=-\frac{\partial U}{\partial q_i}
\end{split}
\end{equation*}
\subsection{Properties of Hamiltonian Dynamics}
The Hamiltonian dynamics have several properties, which are very crucial when used in relation to MCMC simulations. The first property is that the dymaics are reversible
\\
\\
Secondly we have the property of conservation, that is the dynamics keeps the Hamiltionian invariant. This can easily be verified from equation \ref{eq:hamilton_equations}
\begin{equation*}
    \frac{d \mathcal{H}}{d t}=\sum_{i=1}^{d}\left[\frac{d q_{i}}{d t} \frac{\partial \mathcal{H}}{\partial q_{i}}+\frac{d p_{i}}{d t} \frac{\partial \mathcal{H}}{\partial p_{i}}\right]=\sum_{i=1}^{d}\left[\frac{\partial \mathcal{H}}{\partial p_{i}} \frac{\partial \mathcal{H}}{\partial q_{i}}-\frac{\partial \mathcal{H}}{\partial q_{i}} \frac{\partial \mathcal{H}}{\partial p_{i}}\right]=0
\end{equation*}
In the HMC method, we will use Metropolis updates, with proposals found by the Hamiltonian dynamic. We have that the acceptance probability is one if $\mathcal{H}$ is kept invariant (see e.g \cite{neal2012mcmc}). This is however not possible in practise, and we are only able to keep $\mathcal{H}$ approximately invariant, but we will come back to this issue later. The last crucial property of the Hamiltonian dynamics is the volume preservation property, which gives us that the dynamics preserves volume in the $(q,p)$ space.

\begin{equation*}
    \sum_{i=1}^{d}\left[\frac{\partial}{\partial q_{i}} \frac{d q_{i}}{d t}+\frac{\partial}{\partial p_{i}} \frac{d p_{i}}{d t}\right]=\sum_{i=1}^{d}\left[\frac{\partial}{\partial q_{i}} \frac{\partial \mathcal{H}}{\partial p_{i}}-\frac{\partial}{\partial p_{i}} \frac{\partial \mathcal{H}}{\partial q_{i}}\right]=\sum_{i=1}^{d}\left[\frac{\partial^{2} \mathcal{H}}{\partial q_{i} \partial p_{i}}-\frac{\partial^{2} \mathcal{H}}{\partial p_{i} \partial q_{i}}\right]=0
\end{equation*}

\subsection{The Leapfrog Method}
We will now turn to how we should simulate the differential equation in equation \ref{eq:hamilton_equations}. A natural way is to approximate the equation by discretizing time, using some small stepsize, which we will denote $\epsilon$.
For HMC the Leapfrog integrator scheme is often proposed (see e.g \cite{betancourt2017conceptual} or \cite{neal2012bayesian}), first we update the momentum variables for a half step $\epsilon/2$, then we do a full step for the position variables, where the updated values for the momentum variables are used and hereafter do a final half step for the momentum variables using the updated position variables. Formally we can write this as follows,
\begin{equation*}
\begin{split}
p_{i}(t+\varepsilon / 2) &=p_{i}(t)-(\varepsilon / 2) \frac{\partial U}{\partial q_{i}}(q(t)) \\
q_{i}(t+\varepsilon) &=q_{i}(t)+\varepsilon \frac{p_{i}(t+\varepsilon / 2)}{m_{i}} \\
p_{i}(t+\varepsilon) &=p_{i}(t+\varepsilon / 2)-(\varepsilon / 2) \frac{\partial U}{\partial q_{i}}(q(t+\varepsilon))
\end{split}
\end{equation*}


\subsection{The Hamiltonian Monte Carlo Algorithm}


\subsection{No-U-Turn sampler}

\section{Variational InferenceMethods}
\subsection{Automatic Variational Inference}\label{sec:ADVI}
\subsection{Bayes by Backprop}
\textcolor{red}{Maybe this section should be something we could mention as for future work? as the PYMC VI implementation is build on ADVI (\ref{sec:ADVI}) and not Bayes by Backprop? Otherwise we should check if we can build this our selfs? I have not yet seen it implemented.}
