\chapter{Machine Learning Basics}
Bayesian neural networks are a specific kind of neural network which is a type of machine learning algorithms. So in order to understand Bayesian neural networks one must first know the basic principles of machine learning. This chapter provides a brief introduction to the fundamental theory of machine learning, that is used throughout the rest of this dissertation. \\
\\
Machine learning is the study of computer algorithms that learn by analyzing data. Most machine learning algorithms can be divided into the two categories of supervised learning and unsupervised learning. We describe the informal distinction between these and the most common tasks used for supervised learning. As neural networks are considered supervised learning the following sections focuses only on theory for supervised learning algorithms.\\
\\
We proceed to describe the challenge of finding patterns and generalizing these to new data while describing the various machine learning components such as capacity, loss functions and regularization. Most machine learning algorithms are based on the idea of minimizing the loss function using an optimization algorithm. One of the most common optimization algorithms used in the context of machine learning is called stochastic gradient descent, which we cover in section \ref{sec:sgd}. We will later use the theory of stochastic gradient descent in combination with backpropagation in section \ref{sec:backprop} for describing learning in neural networks.\\
\\
As machine learning is a form of applied statistics that focuses on computer algorithms to make statistical estimates we present the two central approaches to statistics: frequentist estimators and Bayesian inference, with emphasis on the latter as this is the approach used in Bayesian neural networks.

\section{Supervised learning}
Machine learning algorithms are typically categorized by being either supervised or unsupervised. Supervised learning use labelled examples as targets to create prediction rules (also called hypothesis) for learning the label of new examples. Unsupervised learns useful properties of the structure of the data without being provided target labels to predict. However as mentioned by \cite{Goodfellow-et-al-2016} the distinction is not formally defined since there is no objective way to determine whether a value is a feature or a target label.\\
\\
One example of a learning algorithm that is typically viewed as supervised is classic linear regression that uses examples $x$ and their labels $y$ to create a linear function to determine $y$-values of new examples $x$. An example of a learning algorithm one typically views as unsupervised is $k$-means clustering, that divides a training set into $k$ clusters based on the distance of the examples. \\
\\
As Bayesian Neural Networks uses target labels to predict labels of new examples it is considered a supervised learning algorithm. Therefore the following chapter will only cover the machine learning basics of supervised algorithms. \\
\\
Typical tasks of supervised machine learning algorithms are:
\begin{itemize}
    \item Classification when the output space $\YY$ are labels. This is called binary classification when we work with 2 labels. In this case the algorithm must learn to separate examples of these 2 labels. Binary labels (be it male or female, yes or no etc.) are typically translated to a numerical representation by taking $\YY = \{ \pm 1 \}$ or $\YY = \{0,1 \}$. When working with more than 2 but a finite amount of labels we call the task multiclass classification. Depending of the setting and amount of labels it can be preferable to use regression instead of multiclass classification.
    \item Regression when the output space $\YY = \mathbb{R}$. For example predicting someones weights could be modeled as a regression problem.
    \item Structured prediction when the output is not merely a number or a label, but a structured object. An example of this is machine translation where the input is a sentence in one language and the output a sentence in another language. 
\end{itemize}


\section{Loss Functions} \label{sec:loss_func}
In order to evaluate the performance of a supervised machine learning algorithm we use a quantitative measure called a loss function. The loss function is an encoding, of how much the algorithm should care about making certain kinds of mistakes, and it is based on this measure that the algorithm selects a hypothesis $h$ from a set of possible hypothesis $\mathbb{H}$ called the hypothesis space. A hypothesis (also called prediction rule) can be understood as a recipe, that the algorithm develops to perform it's task. An example would be the weights $\boldsymbol{w}$ for performing the linear regression $\hat{y}^{(i)} = \boldsymbol{w}^\top \boldsymbol{x}^{(i)}$. Another example would be a specific range of values for a specific set of features for classification, like a set of RGB color-values to classify if an input-image is depicting an apple or a pear. We treat the hypothesis as a function $h\lr{\boldsymbol{x}^{(i)}}$ taking $\boldsymbol{x}^{(i)}$ as input and predicting $\hat{y}^{(i)}$. We define the loss function $\ell \lr{h \lr{\boldsymbol{x}^{(i)}}, y^{(i)}}$ as being the loss of predicting $h \lr{\boldsymbol{x}^{(i)}}$ when the target label is $y^{(i)}$.\\ 
\\
We want the algorithm to return the "best" hypothesis $h^*$, which in this context should be interpreted as the hypothesis that gives the least amount of expected loss on new samples $\lr{\mathbf{x}^{(i)}_{\text{new}}, \text{y}^{(i)}_{\text{new}}}$
\begin{equation*}
    L(h) \equiv \E \lrs{\ell \lr{h \lr{\mathbf{x}_{\text{new}}^{(i)}}, \text{y}_{\text{new}}^{(i)}}}
\end{equation*}
where the expectation is taken with respect to the data distribution $p_{\text{data}}$. If such a value is satisfyingly low, it means that $h^*$ is a useful tool for performing its task on new data. We write this formally as
\begin{equation*}
   h^* = \argmin_{h \in \mathbb{H}} L(h)
\end{equation*}
\\
A typical loss function used for binary classification is the zero-one loss 
$$ \ell\left(h\lr{\boldsymbol{x}^{(i)}}, y^{(i)}\right)=\1\left(h\lr{\boldsymbol{x}^{(i)}} \neq y^{(i)} \right)=\left\{\begin{array}{ll}
0, & \text { if } h\lr{\boldsymbol{x}^{(i)}}=y^{(i)} \\
1, & \text { otherwise }
\end{array}\right. $$
giving $0$ loss for when the predicted label $h\lr{\boldsymbol{x}^{(i)}}$ is equal to the true label $y^{(i)}$ and $1$ otherwise. This loss function trains the algorithm to generate hypothesis, that makes the least number of mistakes. \\
\\
Common loss functions in regression are squared loss (also called L2 loss)
\begin{equation} \label{eq:mse}
    \ell\left(h\lr{\boldsymbol{x}^{(i)}}, y^{(i)} \right) = \lr{ h\lr{\boldsymbol{x}^{(i)}} - y^{(i)} }^2
\end{equation}
or absolute loss (also called L1 loss)
$$ \ell\left(h\lr{\boldsymbol{x}^{(i)}}, y^{(i)}\right) = \lra{h\lr{\boldsymbol{x}^{(i)}} - y^{(i)}}$$
These functions trains the algorithm to create hypothesis that minimizes the differences from the target labels. \\
\\
A loss function used in a Bayesian setting is the Kullback-Leibler (KL) divergence
\begin{equation}
    D_{\mathrm{KL}}(P \| Q)=\mathbb{E}_{\mathrm{x} \sim P}\left[\begin{array}{c}
P(x) \\
\log _{Q(x)}
\end{array}\right]=\mathbb{E}_{\mathrm{x} \sim P}[\log P(x)-\log Q(x)]
\end{equation}
This function measures how different two distributions $P(x)$ and $Q(x)$ are over the same random variable $x$. The KL divergence has the useful properties of being non-negative and equal to $0$ if and only if $P$ and $Q$ are the same distribution for discrete variables and equal to $0$ almost everywhere for continuous variables. \cite{Goodfellow-et-al-2016} shows how the KL divergence is not symmetric $D_{\text{KL}}(P \| Q) \neq D_{\text{KL}}(Q \| P)$ meaning that there are important consequences to the choice of using $D_{\text{KL}}(P \| Q)$ or $D_{\text{KL}}(Q \| P)$. \textcolor{red}{Revisit.}
\\
\\
These are some of the convenient general choices, and there are many more. But such general loss functions is not necessarily the right choice for a particular application. For example if a firm knows the exact monetary loss of packing a wrong item as a result of a misclassification happening in a robotic packaging system. Then it might be beneficial to use a table (perhaps formally written as a sum of indicatorfunctions) of the costs of packaging each of their items as a loss function instead of using the general ones found in the literature.

\section{Training \& Validating}
The central challenge in machine learning is making sure the algorithm creates hypothesis that perform well on new data that was not used in selecting the hypothesis. As mentioned in section \ref{sec:loss_func} this is done by minimizing $L(h)$, but as we don't know $p_\text{data}$ (if we did we wouldn't need the algorithm) we can't evaluate $L(h)$. We must instead approximate $L(h)$ by it's empirical estimate 
\begin{equation*}
    \hat{L}(h,S)= \frac{1}{N} \sum_{i=1}^N \ell \lr{h\lr{\boldsymbol{x}^{(i)}}, y^{(i)}}
\end{equation*}
with $\lr{ h\lr{\mathbf{x}^{(i)}}, \text{y}^{(i)}} \in S$ for $i = 1, \dots N$.\\
\\
However when we select a hypothesis $\hat{h}^*_S$ in $\mathbb{H}$ based on empirical loss $\hat{L} \lr{h, S}$ then the loss of this hypothesis $\hat{h}^*_S$ becomes a biased estimate of $L(\hat{h}^*_S)$. This is because $\hat{h}^*_S$ is selected based on the minimum empirical error on $S$, so from the perspective of $\hat{h}^*_S$ new samples might not be interchangeable with samples in $S$, since including these new samples could result in a different hypothesis that minimizes the loss on $S$. \\
\\
To get an unbiased estimate of $L(\hat{h}^*_S)$ for evaluating a model it is common practice to split the sample set $S$ into a training set $S_\text{train}$ and a validation $S_\text{val}$. One can then find the best hypothesis for the training set $h^*_\text{train}$ and use the validation set for computation of $\hat{L}\lr{h^*_\text{train}, S_\text{val}}$ which he can then used to evaluate the performance. Based on the assumption that new samples $\lr{\mathbf{x}^{(i)}_{\text{new}}, \text{y}^{(i)}_{\text{new}}}$ are distributed identically with the samples in $S_\text{val}$ then these new samples are exchangeable with the ones in $S_\text{val}$ from the perspective of $h^*_\text{train}$. This means that $\E \lrs{\ell \lr{h^*_\text{train} \lr{\boldsymbol{x}^{(i)}}, \boldsymbol{y}^{(i)}}} = \E \lrs{\ell \lr{h^*_\text{train} \lr{\mathbf{x}^{(i)}_{\text{new}}}, \text{y}_{\text{new}}^{(i)}}}$ and therefore $\hat{L}\lr{h^*_\text{train}, S_\text{val}}$ is an unbiased estimate of $L(h^*_\text{train})$.

\subsection{Cross-Validation} \label{sec:cv}
Splitting a dataset into a training set and a validation set can be problematic if the resulting validation set is too small. Such a small validation set will give rise to statistical uncertainties around the estimated average loss and will make it problematic to evaluate the model. When we have large dataset this is not a problem, but when we work with small datasets it can become a serious issue. \\
\\
A workaround for this issue is to use cross-validation. Cross-validation repeats the training and testing procedure on different randomly chosen splits of the original dataset. This enables us to use more examples for estimating the average test loss for the cost of computational power. \\
\\
A common method for cross-validation is the $K$-fold cross-validation that splits the dataset into $K$ non-overlapping and roughly equally sized subsets. On trial $i$ the $i$th subset is used as the validation set while the rest $K - 1$ subsets is used as the training set. The validation loss is then estimated by the cross-validation loss computed as the average validation loss across the $K$ trials.\\
\\
One problem with this approach is however measuring the uncertainty since there is no unbiased estimators of the variance of the average error estimators as mentioned by \cite{Bengio04}, so one must instead use approximations. Another discussed issue is how the choose the value for $K$. This problem does not have a clear answer but 5- or 10-fold cross-validation are generally recommended as a good compromise, see \cite{brieman_spector_1992} and \cite{kohavi_1992}.

\section{Overfitting \& Underfitting}
The average loss attained on the training set when selecting the best hypothesis $\hat{L}\lr{h^*_\text{train}, S_\text{train}}$ is what we will call the training error. The average loss attained from the validation set $\hat{L}\lr{h^*_\text{train}, S_\text{val}}$, used for validating the model, is what we will call validation error.\\
\\
Having a low training error means that we have made a prediction rule that fit our training set well. Having a small gap between training and validation error means that the prediction rule generalizes well on new data, which is why the validation error is also sometimes called generalization error. These factors are what corresponds to underfitting and overfitting, which is used to measure performance of the model.\\
\\
Underfitting occurs when the model is not able to obtain a sufficiently low training error. This is seen in figure \ref{fig:regr_example} in the case of linear regression of degree 1, where the model fitted on the training data lies far from the training samples, meaning it must have a large mean square training error. Overfitting occurs when the gap between the training error and validation error is too large. This large gap indicates that the model faithfully reflects idiosyncrasies of the training data rather than the true underlying process generating the data. This can be seen in figure \ref{fig:regr_example} in the case of linear regression of degree 15, where the model lies very close to all of the training samples meaning it must have a very small mean square training error. But if we sample points from the true function we see that many of these very likely will lie far from the model, thus resulting in a large test error relative to the training error. Overfitting and underfitting are both something we want to avoid. \\
\\
We can control whether a model under- or overfits by changing it's capacity. Capacity is a model's ability to learn a larger variety of hypothesis. A model with a low capacity might underfit the training set if it can't capture the structure of the data. A model with high capacity might overfit if it learns properties of the training set that isn't representative of the validation set. As an example we can increase the capacity of linear regression by allowing it to fit higher order polynomials, thus increasing the hypothesis space from which we draw prediction rules. This example is seen in figure \ref{fig:regr_example}, where we see that a low capacity causes underfitting while a very large capacity causes the model to overfit.

\begin{figure}[h!]
    \centering
    \includegraphics[width=\textwidth,height=\textheight,keepaspectratio]{reg_example.png}
    \caption{Regression of different degree polynomials on a $\cos$ curve. We see that a linear regression of degree 1 is insufficient to fit the training set and is underfitting. A polynomial of degree 4 fits the curve almost perfectly while a polynomial of degree 15 learns the noise of the data and overfits. How well the different regressors generalize to a validation set is evaluated quantitatively by calculating the mean square error using the loss function in equation \ref{eq:mse} using 10-fold cross-validation. We see that this error is lowest when using 4-degree polynomial regression. Cross-validation is covered in section \ref{sec:cv}. The standard error of these losses are shown in the parenthesis.}
    \label{fig:regr_example}
\end{figure}

\section{Regularization}
So far we have covered how to control over- and underfitting by changing a model's capacity. Another method is regularization which encodes the objective function with preferences for certain functions instead of excluding functions from the hypothesis space. The objective function is what we ask a machine learning algorithm to minimize, and has in the previous been synonymous with average loss. \\
\\
A popular type of regularization is norm penalties, where a norm penalty function $\Omega(\boldsymbol{\theta})$ of model parameters $\boldsymbol{\theta}$ is added to the objective function  $J$ (which is usually average loss). Then the regularized objective function is
\begin{equation} \label{eq:reg_obj_func}
    \Tilde{J}(\boldsymbol{\theta}; \mathbf{X}, \mathbf{y}) = J(\boldsymbol{\theta}; \mathbf{X}, \mathbf{y}) + \alpha \Omega(\boldsymbol{\theta})
\end{equation}
where $\alpha \in [0, \infty]$ is a parameter chosen before minimizing $\Tilde{J}$, that controls how much the penalty term $ \Omega(\boldsymbol{\theta})$ contributes relatively to the objective function $J$. When $\alpha=0$ we have no regularization while larger values for $\alpha$ results in more regularization. In this way minimizing $J(\mathbf{w})$ becomes a trade-off between fitting the training data and having small weights.\\
\\
A common norm penalty function is the $L^2$ parameter norm penalty also known as weight decay $\Omega \lr{\boldsymbol{\theta}} = \frac{1}{2} \lra{\lra{\boldsymbol{w}}}_2^2$. $L^2$ regularization is also known as ridge regression or Tikhonov regularization. If we assume no bias parameter and that model parameters are only weights $\boldsymbol{\theta} = \boldsymbol{w}$ we get the regularized objective function
\begin{equation*} 
    \Tilde{J}(\boldsymbol{w}; \mathbf{X}, \mathbf{y}) = J(\boldsymbol{w}; \mathbf{X}, \mathbf{y}) + \frac{\alpha}{2} \boldsymbol{w}^\top \boldsymbol{w}
\end{equation*}
We can clearly see from this that larger values of $\alpha$ punishes larger weights. \\
\\
As shown by \cite{Goodfellow-et-al-2016} L1 regularization often leads to more sparse solutions than L2 regularization. A sparse solution is one that require fewer variables and this happens more frequently with L1 loss as it results in more variables being 0.\\
\\
There are various other techniques to reduce overfitting, which we will not cover. One such related type of regularization is norm penalties as constrained optimization problems where $\Omega \lr{\boldsymbol{\theta}}$ is constrained by some constant $k$ while seeking to minimize equation \ref{eq:reg_obj_func}. Another one who gets a brief mention is data augmentation that increases the size of the training set by augmenting existing data and adding the augmented copy to the dataset. An example for a dataset of images is rotating, scaling or shifting an image, then adding it to the dataset along with the original. Other less general methods are \textcolor{red}{early stopping (add as section?)} that are used for iterative methods and dropout, covered in section \ref{sec:dropout}, used for neural networks. 


\section{Stochastic Gradient Descent} \label{sec:sgd}
Most Machine Learning algorithms is based on an optimization problem, where we wish to minimize some object function. We will consider samples from the sample space $\mathbb{X}$ and our goal is to estimate the model parameters $\boldsymbol{\theta}$. The goal is to minimize the loss function  $J(\boldsymbol{\theta},\boldsymbol{X},y)$ and since it is a convex function, the function is minimized where the gradient is zero
\begin{equation*}
    \nabla J(\boldsymbol{\theta},\boldsymbol{X},y)=\left[\frac{\partial J}{\partial\theta_1},\frac{\partial J}{\partial\theta_2},\ldots,\frac{\partial J}{\partial\theta_d}\right]=\boldsymbol{0}
\end{equation*}
The gradient tells us which direction that has the steepest slope and we can thus change the model parameters such that we move in the opposite direction of where the gradient is pointing, that is in the direction that locally decreases the objective function the fastest. In order to run the algorithm we need to initialize the model parameters $\boldsymbol{\theta}:=\boldsymbol{\theta}^0$, evaluate the objective function $J(\boldsymbol{\theta},\boldsymbol{X},y)$ and calculate the gradient respectively $\nabla J(\boldsymbol{\theta},\boldsymbol{X},y)$. Further we also need to specify a learning rate $\eta$, which determines the size of the step in each iteration of the gradient descent algorithm. The model parameters are then updated iteratively as follows
\begin{equation*}
    \boldsymbol{\theta}^{(\tau+1)}=\boldsymbol{\theta}^{(\tau)}-\eta \nabla J(\boldsymbol{\theta}^{(\tau)},\boldsymbol{X},y)
\end{equation*}
Note that when we are far away from optimum (large gradient) we are taking larger steps, but when we get closer to optimum (smaller gradient) we take smaller and smaller steps. The algorithm is not assured to reach a global minimum unless the loss function is strictly convex. As mentioned in \cite{bishop2007}, that in order to find a "good" minimum, we might be forced to run the algorithm multiple times, where we each time initialize with a randomly chosen starting point $\boldsymbol{\theta}^0$. Further to algorithm is calculating the gradient based on the entire sample in each iteration, this can make the computational burden very large. \\
\\
A more delicate way is to use the Stochastic gradient descent (SGD) algorithm, which is a stochastic approximation of the gradient descent algorithm described above, where it replaces the gradient calculated on the entire data set by an estimate calculated from a randomly selected data point $\boldsymbol{x}^{(i)}$. This is fortunate when one faces high-dimensional optimization problems as it reduces the computational burden significantly (\cite{bottou2008tradeoffs}). Algorithm \ref{algo_1} is the SGD algorithm written i pseudo code.\\
\begin{algorithm}[H]\label{algo_1}
\SetAlgoLined
\KwInput{a data set $S$}
\KwOutput{Model parameters $\boldsymbol{\theta}$}
 initialize $\boldsymbol{\theta}\leftarrow \boldsymbol{\theta}^0$\;
\Repeat{stopping criteria is met}{$\operatorname{pick}(\boldsymbol{x}^{(i)}, y^{(i)}) \in S$\;
$\boldsymbol{\theta} \leftarrow \boldsymbol{\theta} - \left.\nabla J\left(\boldsymbol{\theta},\boldsymbol{x}^{(i)},y^{(i)}\right)\right|_{\boldsymbol{\theta}}$\;
}
 \caption{Stochastic Gradient Descent}
\end{algorithm}







\section{Bayesian Statistics}
There are two main approaches in statistical learning theory: the Frequentistic and the Bayesian. The ideas behind Bayesian statistics goes back to 18th century (\cite{stigler1986history}). The theory considers probability to reflect uncertainties. The process in Bayesian statistics is to fit a probability model to a data set $S$ and summarize it by a probability distribution on the model parameters $\boldsymbol{\theta}$ and predictions for new observations $S^{new}$. To make any probability statements about $\boldsymbol{\theta}$ given $S$, we need the posterior distribution of $\boldsymbol{\theta}$, that is the probability distribution of the parameter given the data $p(\boldsymbol{\theta} \mid S)$. The posterior distribution is found by considering the joint distribution of our parameter and data set
\begin{equation*}
    p(\boldsymbol{\theta},S)=p(\boldsymbol{\theta})p(S|\boldsymbol{\theta})
\end{equation*}
and then dividing both sides by $p(S)$ which gives us
\begin{equation*}
         \frac{p(\boldsymbol{\theta},S)}{p(S)}=\frac{p(\boldsymbol{\theta})p(S|\boldsymbol{\theta})}{p(S)}
\end{equation*}
then we note that $\frac{p(\boldsymbol{\theta},S)}{p(S)}=p(\boldsymbol{\theta}|S)$ and we have the posterior distribution given as 
\begin{equation*}
         p(\boldsymbol{\theta}|S)=\frac{p(\boldsymbol{\theta})p(S|\boldsymbol{\theta})}{p(S)}
\end{equation*}
The prior, $p(\boldsymbol{\theta})$, reflects our knowledge about the data before we observe it, $p(S|\boldsymbol{\theta})$ is the likelihood function and $p(S)$ is called the model evidence. The posterior distribution represents our updated knowledge about $\boldsymbol{\theta}$ after we see the data. \\
\\
The Bayesian method uses a full distribution over $\boldsymbol{\theta}$ to make predictions. Let us for example consider the case where we have observed $n$ samples $S=\{x^{(1)},x^{(2)},\ldots,x^{(n)}\}$, to make a prediction about an unobserved quantity $S^{new}=x^{(n+1)}$ we need the 
posterior predictive distribution (\cite{gelmanbda04})
\begin{equation*}
p\left(x^{(m+1)} \mid x^{(1)}, \ldots, x^{(m)}\right)=\int p\left(x^{(m+1)} \mid \boldsymbol{\theta}\right) p\left(\boldsymbol{\theta} \mid x^{(1)}, \ldots, x^{(m)}\right) d \boldsymbol{\theta}
\end{equation*}
unlike the maximum likelihood method, that uses a point estimate for $\boldsymbol{\theta}$ to make predictions on any unobserved data $S^{new}$, the Bayesian method takes the uncertainty of estimating $\boldsymbol{\theta}$ into account. For classification tasks we often consider a labeled sample $S=\{\boldsymbol{X},\boldsymbol{Y}\}$, in that case the posterior predictive distribution is
\begin{equation}\label{eq: posterior predective distribution_2}
\begin{split}
        P\left(y^{(n+1)} \mid x^{(n+1)},\left(x^{(1)}, y^{(1)}\right), \ldots,\left(x^{(n)}, y^{(n)}\right)\right)
=\int P\left(y^{(n+1)} \mid x^{(n+1)}, \boldsymbol{\theta}\right) P\left(\boldsymbol{\theta} \mid\left(x^{(1)}, y^{(1)}\right), \ldots,\left(x^{(n)}, y^{(n)}\right)\right) d \boldsymbol{\theta}
\end{split}
\end{equation}
The integral in equation \ref{eq: posterior predective distribution_2} is often intractable and thus must be approximated by some method, which will be elaborated  later.
\section{Monte Carlo Methods}
% Ved ikke om dette afsnit skal skubbes hen under BNN? Men skriver her og så kan vi rykke det senere. 
When an integral is intractable, we turn to approximations techniques such as Monte Carlo methods. The idea behind Monte Carlo methods is to view the integral as an expectation of some random variable with respect to a probability distribution $p(\cdot)$. Let us consider the case where we have a random variable $\mathbf{x}$ and some function $f(\mathbf{x})$ and we want to approximate the following integral as an expectation under the probability distribution $p(\mathbf{x})$ 
\begin{equation*}
    s=\int p(\mathbf{x}) f(\mathbf{x}) d \mathbf{x}=\mathbb{E}_{p}[f(\mathbf{x})]
\end{equation*}
Now in order to approximate $s$ we can draw samples from the distribution $p(\mathbf{x})$ and approximate the expected value by the empirical average. If we for example draw $n$ samples $\mathbf{x}\sim p(\mathbf{x})$ we can approximate $s$ by $\hat{s}_n$
\begin{equation*}
        \hat{s}_{n}=\frac{1}{n} \sum_{i=1}^{n} f\left(\mathbf{x}^{(i)}\right)
\end{equation*}
It is thus possible to a approximate the theoretical expected value, by then the empirical mean. This implies the simplest situation, that is, where it is possible to simulate directly from the density, we later see cases where this is not possible.\\
\\
We can justify this approximation, by noticing that $\hat{s}_n$ is an unbiased estimator of $s$
\begin{equation*}
    \mathbb{E}\left[\hat{s}_{n}\right]=\frac{1}{n} \sum_{i=1}^{n} \mathbb{E}\left[f\left(\mathbf{x}^{(i)}\right)\right]=\frac{1}{n} \sum_{i=1}^{n} s=s
\end{equation*}
This method can further be supported by the law of large numbers, which states that if the samples $\mathbf{x}^{(i)}$ are independent and identical distributed (i.i.d), the the empirical average converges to the true expectation
\begin{equation*}
    \lim _{n \rightarrow \infty} \hat{s}_{n}=s
\end{equation*}
this only holds if the variance of the individual terms $\operatorname{Var}[f(\mathbf{x}^{(i)}]$ is bounded. To see this more clearly, we note that $\operatorname{Var}[\hat{s}_n]$ converges to 0 if an only if $\operatorname{Var}[f(\mathbf{x}^{(i)}]<\infty$
\begin{equation*}
    \begin{split}
\operatorname{Var}\left[\hat{s}_{n}\right] &=\frac{1}{n^{2}} \sum_{i=1}^{n} \operatorname{Var}[f(\mathbf{x})] \\
&=\frac{\operatorname{Var}[f(\mathbf{x})]}{n}
\end{split}
\end{equation*}
Further the central limit theorem states that, if $\mathbb{E}[f(\mathbf{x})]=s$ and $\operatorname{Var}[f(\mathbf{x})]<\infty$ then
\begin{equation*}
    \frac{\hat{s}_{n}-s}{\sqrt{\operatorname{Var}[f(\mathbf{x})] / n}} \stackrel{D}{\rightarrow} \mathcal{N}(0,1)
\end{equation*}
which is equivalent to
\begin{equation*}
    \hat{s}_n\sim \mathcal{N}\left(s,\frac{\operatorname{Var}[f(\mathbf{x})]}{n}\right)
\end{equation*}
When it is not possible to make simulations of $\mathbf{x}$ directly, Markov Chain Monte Carlo methods are used (\textcolor{red}{see section XYXZZÆØÅ}), which in short is based on simulating from a target distribution by setting up a Markov chain to converge to a specific distribution.




